{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aldao Amoedo, Héctor\n",
    "\n",
    "Cabaleiro Pintos, Laura\n",
    "\n",
    "Cotardo Valcárcel, Donato José\n",
    "\n",
    "Romero Conde, José"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   _Librerias_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Pkg\n",
    "Pkg.add(\"CSV\")\n",
    "Pkg.add(\"DataFrames\")\n",
    "Pkg.add(\"Statistics\")\n",
    "Pkg.add(\"StatsBase\")\n",
    "Pkg.add(\"Random\")\n",
    "Pkg.add(\"ScikitLearn\")\n",
    "Pkg.add(\"Plots\")\n",
    "Pkg.add(\"MLBase\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: could not import CrossValidation.StratifiedGroupKFold into Main\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PyObject <class 'sklearn.svm._classes.SVC'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using CSV\n",
    "using DataFrames\n",
    "using Plots\n",
    "using Statistics\n",
    "using Random\n",
    "using ScikitLearn\n",
    "using ScikitLearn.CrossValidation: cross_val_score, KFold\n",
    "#using MLBase: StratifiedGroupKFold\n",
    "using ScikitLearn.Pipelines: Pipeline, named_steps, FeatureUnion\n",
    "using ScikitLearn.GridSearch: GridSearchCV \n",
    "@sk_import decomposition: (PCA, FastICA)\n",
    "@sk_import discriminant_analysis: LinearDiscriminantAnalysis\n",
    "@sk_import ensemble: (AdaBoostClassifier, BaggingClassifier, GradientBoostingClassifier)\n",
    "@sk_import feature_selection: (SelectKBest, f_classif, mutual_info_classif, RFE)\n",
    "@sk_import impute: SimpleImputer\n",
    "@sk_import linear_model: LogisticRegression\n",
    "@sk_import manifold: (LocallyLinearEmbedding, Isomap)\n",
    "@sk_import neighbors: KNeighborsClassifier\n",
    "@sk_import neural_network: MLPClassifier\n",
    "@sk_import preprocessing: MinMaxScaler\n",
    "@sk_import svm: SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparación de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El punto 1, está completado, aunque creo que el 1.3, donde preparamos los datos transformandolos y rellanando los nulos está mal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Cargar los datos y descripción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = CSV.read(\"Datos_Practica_Evaluacion_1.csv\", DataFrame)\n",
    "\n",
    "num_instancias, num_variables = size(df)\n",
    "num_individuos = length(unique(df[:, 1]))\n",
    "num_clases_salida = length(unique(df[:, end]))\n",
    "\n",
    "println(\"Número de variables: $num_variables\")\n",
    "println(\"Número de instancias: $num_instancias\")\n",
    "println(\"Número de individuos: $num_individuos\")\n",
    "println(\"Número de clases de salida: $num_clases_salida\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los datos dados ya están cargados, y como podemos observar tienen:\n",
    "* 563 Variables\n",
    "* 10299 Instancias\n",
    "* 30 Individuos\n",
    "* 6 Clases de salida"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Calcular porcentaje de nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nulos_totales = 0\n",
    "\n",
    "for col ∈ names(df)\n",
    "    num_nulos = count(ismissing, df[:, col])\n",
    "    num_nulos_totales += num_nulos\n",
    "    porcentaje_nulos = (num_nulos / num_instancias) * 100\n",
    "end\n",
    "\n",
    "porcentaje_nulos_totales = (num_nulos_totales / (num_instancias * num_variables)) * 100\n",
    "println(\"Porcentaje total de nulos en el conjunto: $porcentaje_nulos_totales%\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Preparar los datos para las técnicas de clasificación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para rellenar valores faltantes tenemos que hacernos una idea de que tipo de datos encontraremos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col ∈ names(df)\n",
    "    if eltype(df[!, col]) == Union{Missing, Float64}\n",
    "        df[ismissing.(df[!, col]), col] .= mean(skipmissing(df[!, col]))\n",
    "        df[!, col] = Float64.(df[!, col]) \n",
    "    end\n",
    "end\n",
    "\n",
    "println(\"Valores nulos rellenados.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Set([eltype(df[!,col]) for col in names(df)])  # ya no hay missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Segmentar el 10% de los datos usando HoldOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Random.seed!(172)\n",
    "\n",
    "holdout_individuos = shuffle(unique(df[:, :subject]))[1:Int(round(0.1 * length(unique(df[:, :subject]))))]  \n",
    "holdout_df = filter(fila -> fila.subject in holdout_individuos, df)\n",
    "train_df = filter(fila -> !(fila.subject in holdout_individuos), df)\n",
    "\n",
    "println(\"Individuos en el holdout: \", holdout_individuos)\n",
    "println(\"Tamaño del conjunto de entrenamiento: $(size(train_df)[1])\")\n",
    "println(\"Tamaño del conjunto de holdout: $(size(holdout_df)[1])\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Fold y escalado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separacion X Y\n",
    "train = Array(train_df)\n",
    "test = Array(holdout_df)\n",
    "\n",
    "X_train = train[:,1:end-1]\n",
    "Y_train = train[:,end]\n",
    "X_test = test[:,1:end-1]\n",
    "Y_test = test[:,end];\n",
    "# k fold\n",
    "especificacionCV = ScikitLearn.CrossValidation.KFold(size(train)[1], n_folds=5)\n",
    "folds = [(train[indicesTrain,:], train[indicesTest,:]) for (indicesTrain, indicesTest) ∈ especificacionCV]\n",
    "# escalado\n",
    "X_train = fit_transform!(MinMaxScaler(), X_train);\n",
    "X_test = fit_transform!(MinMaxScaler(), X_test);\n",
    "folds = [((fit_transform!(MinMaxScaler(), train[:,1:end-1]),Vector(train[:,end])), (fit_transform!(MinMaxScaler(), test[:,1:end-1]),Vector(test[:,end]))) for (train, test) ∈ folds ];\n",
    "\n",
    "# de esta forma:\n",
    "# folds tiene los 5 folds (escalado cada uno independientemente)\n",
    "# folds[1] es un fold\n",
    "# folds[1][1] es el entrenamiento de ese fold\n",
    "# folds[1][1][1] es el X del entrenamiento de ese fold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos Basicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function  buenosDias(df)\n",
    "    indice = argmax(df[!,Symbol(\"Accuracy\")])\n",
    "    return df[indice,:]\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function plot_transformed_data(name::String, reductor, X, y)\n",
    "\n",
    "    X_reducida = fit_transform!(reducer, X, y)\n",
    "    plot = scatter(X_reduced[:, 1], X_reduced[:, 2], group=y, legend=:topright, title=name,\n",
    "                xlabel=\"Componente 1\", ylabel=\"Componente 2\", markersize=5)\n",
    "    return plot\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "julia --quiet practicaFinal.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultadosModelosBasicos = DataFrame(filtrado = String[], reduccion = String[], clasificador = String[], Accuracy = Float64[])\n",
    "plot_distribucion = @layout [a b c; d e]\n",
    "\n",
    "filtrado = Dict(\n",
    "   #\"nada\" => \"passthrough\",\n",
    "   \"anova\" => SelectKBest(score_func=f_classif),\n",
    "   \"mi\" => SelectKBest(score_func=mutual_info_classif),\n",
    "   \"rfe\" => RFE(LogisticRegression(max_iter=10),step=0.5)\n",
    " )\n",
    "\n",
    "reduccion = Dict(\n",
    "   #\"nada\" => \"passthrough\",\n",
    "   \"pca\" => PCA(),\n",
    "   \"lda\" => LinearDiscriminantAnalysis(),\n",
    "   \"ica\" => FastICA(),\n",
    "   #\"isomap\" => Isomap(n_neighbors=25),\n",
    "   #\"lle\" => LocallyLinearEmbedding(),\n",
    " )\n",
    "\n",
    "clasificacion = Dict(\n",
    "    \"mlp\" => [MLPClassifier(max_iter=10), Dict(:classifier__hidden_layer_sizes => [[50], [100], [100, 50]])],\n",
    "    \"knn\" => [KNeighborsClassifier(), Dict(:classifier__n_neighbors =>[1, 10, 20])],\n",
    "    \"svm\" => [SVC(), Dict(:classifier__C =>[0.1, 1, 10])]\n",
    ")\n",
    "\n",
    "for (nombreFiltro, filtro) in filtrado\n",
    "    for (nombreReduccion, reduccion) in reduccion\n",
    "        for (nombreClasificador, valor) in clasificacion\n",
    "\n",
    "            clasificador = valor[1]\n",
    "            parametros = valor[2]\n",
    "            modelo = Pipeline([\n",
    "                (\"filtro\", filtro),\n",
    "                (\"reduccion\", reduccion),\n",
    "                (\"classifier\", clasificador) \n",
    "            ])\n",
    "\n",
    "            busqueda = GridSearchCV(modelo, parametros, cv=especificacionCV)\n",
    "            fit!(busqueda, X_train, Y_train)\n",
    "            mejorModelo = busqueda.best_estimator_\n",
    "            mejoresParametros = busqueda.best_params_\n",
    "            accuracy = busqueda.best_score_\n",
    "            Y_pred = predict(mejorModelo, X_test)\n",
    "            accuracy = sum(Y_pred .== Y_test) / length(Y_test)\n",
    "            push!(resultadosModelosBasicos, (nombreFiltro, nombreReduccion, nombreClasificador, accuracy))\n",
    "            println(\"Filtrado: $nombreFiltro,\\nReducción: $nombreReduccion,\\nClasificador: $nombreClasificador,\\nparámetros: $(mejoresParametros),\\nPrecisión: $accuracy\\n\")\n",
    "        \n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "\n",
    "buenosDias(resultadosModelosBasicos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensembles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Adicionalmente, con los datos sólo con el tratamiento de Filtrado ANOVA, recrear\n",
    "las siguientes técnicas\n",
    " - BaggingClassifier con clasificador base KNN con número de vecinos 5 y\n",
    "número de estimadores 10 y 50\n",
    " -  AdaBoosting con estimadores SVM con kernel lineal siendo el número de\n",
    "estimadores 5.\n",
    " -  GBM (GradientBoostingClasifier), con 50 estimadores y un learning_rate de\n",
    "0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ejercicio 10\n",
    "\n",
    "resultadosEjercicio10 = DataFrame(clasificador = String[],  Accuracy = Float64[])\n",
    "\n",
    "clasificadoresEjercicio10 = Dict(\n",
    "    \"bagging10\" => BaggingClassifier(estimator=KNeighborsClassifier(n_neighbors=5), n_estimators=10),\n",
    "    \"bagging50\" => BaggingClassifier(estimator=KNeighborsClassifier(n_neighbors=5), n_estimators=50),\n",
    "    \"adaboosting\" => AdaBoostClassifier(estimator=SVC(kernel=\"linear\"), algorithm=\"SAMME\", n_estimators=5),\n",
    "    \"gbm\" => GradientBoostingClassifier(learning_rate=0.2, n_estimators=50)\n",
    "    )\n",
    "for (i, fold) in enumerate(folds)\n",
    "    println(\"Fold número $i\\n##########\\n\")\n",
    "    for (nombreClasificador, clasificador) in clasificadoresEjercicio10\n",
    "        modelo = Pipeline([\n",
    "            (\"filtro\", SelectKBest(score_func=f_classif)),\n",
    "            (\"classifier\", clasificador)\n",
    "        ])\n",
    "    \n",
    "        fit!(modelo, fold[1][1], fold[1][2])\n",
    "        Y_pred = predict(modelo, fold[2][1])\n",
    "        accuracy = sum(Y_pred .== fold[2][2]) / length(fold[2][1])\n",
    "        push!(resultadosEjercicio10, (nombreClasificador, accuracy))\n",
    "        println(\"Clasificador: $nombreClasificador Precisión: $accuracy\")\n",
    "    end\n",
    "end\n",
    "\n",
    "buenosDias(resultadosEjercicio10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buenosDias(resultadosEjercicio10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Entrenar con el conjunto completo de entrenamiento (todo lo que componía el 5-\n",
    "fold cross-validation) y testear son el 10% reservado\n",
    " - Coger las 5 mejores combinaciones de los modelos anteriores de\n",
    "clasificación, (1 KNN, 1 SVM, 1 MLP, 1 Bagging y 1 AdaBoosting)\n",
    " - Crear un Random Forest con valor para los estimadores del 500 y\n",
    "profundidad máxima de 10\n",
    " - Crear un Hard Voting con las mejores combinaciones del KNN, SVM y MLP\n",
    "(uno para cada una de las técnicas)\n",
    " - Crear un Soft Voting con las mejores combinaciones del KNN, SVM y MLP\n",
    "(uno para cada una de las técnicas) para los pesos coger el porcentaje de\n",
    "acierto en test de cada una de las combinaciones en el 5-fold cross-\n",
    "valiadation\n",
    " - Crear un Ensemble Stacking con MLP como clasificador final, así mismo,\n",
    "use como base las mejores combinaciones del SVM, KNN y MLP\n",
    " - Crear un XGBoost con los valores por defecto\n",
    " - Crear un LightGBM, con los valores por defecto\n",
    " - Crear un Catboost, con los valores por defecto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Normalizar usando MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación de los modelos básicos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Filtrado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1 ANOVA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2 Mutual Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3 RFE con el método de LogisticRegression con una eliminación del 50% de las variables en cada pasada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Reducción dimensionalidad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.1 PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.2 LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.3 ICA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.4 Isomap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.5 LLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Clasificadores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.1 MLP con al menos las siguientes arquitecturas: [50], [100] [100, 50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.2 KNN con valores de vecindario entre 1, 10 y 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.3 SVM con el parámetro C con valores 0.1, 0.5 y 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación de los modelos ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.11.2",
   "language": "julia",
   "name": "julia-1.11"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
